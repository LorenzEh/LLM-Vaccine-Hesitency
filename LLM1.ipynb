{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMe4riYCU3aSraATN6HHEXQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LorenzEh/LLM-Vaccine-Hesitency/blob/main/LLM1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the libraries\n"
      ],
      "metadata": {
        "id": "H30iVz4gctjH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjEOHn3qu9gm"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets peft accelerate bitsandbytes trl accelerate llama-cpp-python huggingface-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the data ready\n",
        "\n",
        "\n",
        "\n",
        "1.   Download the imputed data from google drive\n",
        "2.   Exclude most of the columns (since we're running the code on CPU keeping all of the columns is not feasible)\n",
        "3. Convert pandas dataframe to dictionary, so that it can be converted to a Hugging Face dataset in the next part\n",
        "\n"
      ],
      "metadata": {
        "id": "d5taHxxHR_j8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "m0Fd1kFava5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download(id=\"1VXUSmDyt7qYo5nGpMDWmelTjnWk6KVkb\", output=\"data_imputed_decoded.csv\", quiet=False) # unfortunatly the csv does not work, therefore we need to continue with the stata format\n",
        "data = pd.read_csv(\"data_imputed_decoded.csv\") # copy and paste a single\n"
      ],
      "metadata": {
        "id": "kctU23lHv8MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in data.columns: # print all column names\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "6SxS4iznw3h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of prefixes to include\n",
        "prefixes_to_include = [\"AGE_CAT\", \"EDU_CAT\", \"BULA\", \"GENDER\", \"Aussage\", \"Impfen\"]\n",
        "\n",
        "# function to filter columns based on prefixes\n",
        "def filter_columns_by_prefix(df, prefixes):\n",
        "    filtered_columns = [col for col in df.columns if any(col.startswith(prefix) for prefix in prefixes)]\n",
        "    return df[filtered_columns]\n",
        "\n",
        "# create the new DataFrame with the filtered columns\n",
        "new_df = filter_columns_by_prefix(data, prefixes_to_include)"
      ],
      "metadata": {
        "id": "zw6RC4lZv_Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in new_df.columns:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "O-IfoXSAxS5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df.head(10)"
      ],
      "metadata": {
        "id": "Pva7Ywn_xY30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df['EDU_CAT'] = new_df['EDU_CAT'].str.replace(r' \\(Kat\\. \\d+(-\\d+)?\\)', '', regex=True) # delete the \"(Kat )\" in EDU_CAT"
      ],
      "metadata": {
        "id": "18QkvAjBqHPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Antworten in Text-Format bringen\n",
        "def create_prompt(row):\n",
        "    prompt = \"\"\n",
        "    for question, answer in row.items():\n",
        "        prompt += f'Aussage: \"{question}\" → Antwort: \"{answer}\"\\n'\n",
        "    return {\"text\": prompt.strip()}\n",
        "\n",
        "# Anwendung auf jede Zeile\n",
        "prompts = new_df.apply(create_prompt, axis=1)"
      ],
      "metadata": {
        "id": "_4h0BFxIySq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts.head(10)"
      ],
      "metadata": {
        "id": "mzhRw70h0N6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert dictionary data to sentences using local LLM\n",
        "\n",
        "\n",
        "1.   Parse the dictionary, to make it ready for the chat template\n",
        "2.   Introduce a chat template, this helps the model greatly to understand the input\n",
        "3.   \n",
        "\n"
      ],
      "metadata": {
        "id": "RgplYSKSdFGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# set random seed for reproducibility\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "rziXEnTjewQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing of the raw-data\n",
        "def parse_entry(entry):\n",
        "    text = entry[\"text\"]\n",
        "\n",
        "    # split into key-value pairs\n",
        "    pairs = re.split(r'\\nAussage: ', text)\n",
        "    persona_data = {}\n",
        "    for pair in pairs:\n",
        "        if not pair.strip():\n",
        "            continue\n",
        "        match = re.match(r'^\"(.*?)\" → Antwort: \"(.*?)\"', pair)\n",
        "        if match:\n",
        "            key = match.group(1).replace(\"Aussage: \", \"\").strip()\n",
        "            value = match.group(2).strip()\n",
        "            persona_data[key] = value\n",
        "\n",
        "    # build prompt using key demographics\n",
        "    prompt = f\"\"\"Person:\n",
        "- Geschlecht: {persona_data.get('GENDER', 'Unbekannt')}\n",
        "- Alter: {persona_data.get('AGE_CAT', 'Unbekannt')}\n",
        "- Bildung: {persona_data.get('EDU_CAT', 'Unbekannt')}\n",
        "- Bundesland: {persona_data.get('BULA', 'Unbekannt')}\n",
        "- Impfen Ehestmöglich: {persona_data.get('Impfen: Ehestmöglich', 'Unbekannt')}\n",
        "\"\"\"\n",
        "    response = \"[PLACEHOLDER_FOR_GENERATED_PERSONA]\"\n",
        "    return {\"prompt\": prompt, \"response\": response}"
      ],
      "metadata": {
        "id": "E4lXFjq-f1k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we're getting the data ready by creating a prompt for each person, which can easly be interpreted by the LLM\n",
        "data_dict = {\"prompt\": [], \"response\": []}\n",
        "for entry in prompts:\n",
        "    parsed = parse_entry(entry)\n",
        "    data_dict[\"prompt\"].append(parsed[\"prompt\"])\n",
        "    data_dict[\"response\"].append(parsed[\"response\"])\n",
        "\n",
        "dataset = Dataset.from_dict(data_dict)\n",
        "dataset = dataset.select(range(5)) # we're limiting the dataset size , since we're working on a cpu\n",
        "\n",
        "print(dataset[\"prompt\"][0]) # description of person which is used in the prompt\n",
        "print(dataset[\"response\"][0]) # placeholder"
      ],
      "metadata": {
        "id": "nSGb-CACf6kY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Schritt 2: Modell von Hugging Face herunterladen\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=\"TheBloke/em_german_mistral_v01-GGUF\",\n",
        "    filename=\"em_german_mistral_v01.Q4_K_M.gguf\",\n",
        "    local_dir=\"/content\"  # Speicherort im Colab-Filesystem\n",
        ")\n",
        "\n",
        "# Schritt 3: Modell initialisieren\n",
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=model_path,  # Verwendet den automatischen Pfad\n",
        "    n_ctx=2048,\n",
        "    n_threads=8,           # Nutzt alle CPU-Kerne\n",
        "    n_gpu_layers=0         # 0 = Nur CPU (für GPU: 35+ setzen)\n",
        ")\n"
      ],
      "metadata": {
        "id": "oYz9fPLJEW9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test the model (...)\n",
        "# output = llm(\"Was ist eine Impfung?\", max_tokens=100)\n",
        "# print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "vRQlxpaohvF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FEW_SHOT_EXAMPLES = [\n",
        "    {\n",
        "        \"input\": \"\"\"Person:\n",
        "- Geschlecht: Weiblich\n",
        "- Alter: 30-39 Jahre\n",
        "- Bildung: hoch\n",
        "- Bundesland: Wien\n",
        "- Impfen Ehestmöglich: Trifft eher zu\"\"\",\n",
        "        \"output\": \"Die 30-39-jährige hat einen hohen Bildungsstand, kommt aus Wien und würde sich eher zeitnah impfen lassen.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"\"\"Person:\n",
        "- Geschlecht: Männlich\n",
        "- Alter: 60-69 Jahre\n",
        "- Bildung: mittel\n",
        "- Bundesland: Tirol\n",
        "- Impfen Ehestmöglich: Trifft eher nicht zu\"\"\",\n",
        "        \"output\": \"Der 60-69-jährige Mann aus Tirol hat einen mittleren Bildungsstand und wird sich eher nicht zeitnah impfen lassen.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"\"\"Person:\n",
        "- Geschlecht: Weiblich\n",
        "- Alter: 18-29 Jahre\n",
        "- Bildung: niedrig\n",
        "- Bundesland: Steiermark\n",
        "- Impfen Ehestmöglich: Trifft überhaupt nicht zu\"\"\",\n",
        "        \"output\": \"Die 18-29-jährige Steirerin hat einen niedrigen Bildungsstand und lehnt eine COVID-19-Impfung grundsätzlich ab.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"\"\"Person:\n",
        "- Geschlecht: Weiblich\n",
        "- Alter: 18-29 Jahre\n",
        "- Bildung: niedrig\n",
        "- Bundesland: Oberösterreich\n",
        "- Impfen Ehestmöglich: Teils-teils\"\"\",\n",
        "        \"output\": \"Die 18-29-jährige Oberösterreicherin hat einen niedrigen Bildungsstand und weiß nicht, ob sie sich ehestmöglich impfen lassen würde.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "def format_prompt(prompt):\n",
        "    instruction = \"\"\"[INST] <<SYS>>\n",
        "Du bist ein deutscher Demographie-Experte. Fasse Personendaten in einem Satz zusammen:\n",
        "- Format: [Alter]-jährige/r [Bildung] [Geschlecht] aus [Bundesland] [Impfverhalten]\n",
        "- Beispiel: \"Die 30-39-jährige Akademikerin aus Wien wird sich ehestmöglich impfen lassen.\"\n",
        "- Nur die gegebenen Daten verwenden!\n",
        "<</SYS>>\"\"\"\n",
        "\n",
        "    examples = \"\\n\\n\".join(\n",
        "        f\"{ex['input']}\\nZusammenfassung: {ex['output']}\"\n",
        "        for ex in FEW_SHOT_EXAMPLES\n",
        "    )\n",
        "\n",
        "    return f\"\"\"\n",
        "{instruction}\n",
        "\n",
        "Beispiele:\n",
        "{examples}\n",
        "\n",
        "Aktuelle Daten:\n",
        "{prompt}\n",
        "Zusammenfassung: [/INST]\"\"\"  # Moved [/INST] to end"
      ],
      "metadata": {
        "id": "q5CZ6U_ZFFKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_persona(llm, prompt):\n",
        "    full_prompt = format_prompt(prompt)\n",
        "\n",
        "    output = llm(\n",
        "        full_prompt,\n",
        "        max_tokens=100,        # Increased from 50\n",
        "        temperature=0.6,       # More creativity\n",
        "        top_p=0.95,            # Wider sampling\n",
        "        top_k=40,              # More diverse\n",
        "        repeat_penalty=1.1,    # Reduced penalty\n",
        "        stop=[\"\\n\\n\"],         # Simpler stop condition\n",
        "        echo=False             # Don't return prompt\n",
        "    )\n",
        "\n",
        "    # Handle empty response\n",
        "    response = output['choices'][0]['text'].strip()\n",
        "\n",
        "    # Fallback if empty\n",
        "    if not response:\n",
        "        return \"Zusammenfassung konnte nicht generiert werden.\"\n",
        "\n",
        "    return response.split(\"Zusammenfassung:\")[-1].strip()\n"
      ],
      "metadata": {
        "id": "LvW3LGZ-DeX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testlauf für die ersten 5 Einträge\n",
        "for i in range(5):\n",
        "    prompt = dataset[i][\"prompt\"]\n",
        "    response = generate_persona(llm, prompt)\n",
        "\n",
        "    print(f\"Prompt {i+1}:\")\n",
        "    print(prompt)\n",
        "    print(\"\\nGenerierte Persona:\")\n",
        "    print(response)\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "EzQDfTxqH05f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rest"
      ],
      "metadata": {
        "id": "CJE5tyDnIPxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# --- Konfiguration ---\n",
        "MODEL_NAME = \"dbmdz/german-gpt2\"\n",
        "DEVICE = \"cpu\"\n",
        "\n",
        "GENERATION_CONFIG = {\n",
        "    \"max_new_tokens\": 50,\n",
        "    \"do_sample\": True,\n",
        "    \"temperature\": 0.7,\n",
        "    \"top_k\": 40,\n",
        "    \"top_p\": 0.9,\n",
        "    \"repetition_penalty\": 1.5,\n",
        "    \"no_repeat_ngram_size\": 2,\n",
        "    \"stop_sequence\": \"\\n\"\n",
        "}\n",
        "\n",
        "FEW_SHOT_EXAMPLES = [\n",
        "    {\n",
        "        \"input\": \"\"\"Person:\n",
        "- Geschlecht: Weiblich\n",
        "- Alter: 30-39 Jahre\n",
        "- Bildung: Hoch\n",
        "- Bundesland: Wien\n",
        "- Impfen Ehestmöglich: Trifft eher zu\"\"\",\n",
        "        \"output\": \"Die 30-39-jährige hochgebildete Frau aus Wien tendiert zu einer baldigen Impfung.\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"\"\"Person:\n",
        "- Geschlecht: Männlich\n",
        "- Alter: 60-69 Jahre\n",
        "- Bildung: Mittel\n",
        "- Bundesland: Tirol\n",
        "- Impfen Ehestmöglich: Trifft eher nicht zu\"\"\",\n",
        "        \"output\": \"Der 60-69-jährige Mann mit mittlerer Bildung aus Tirol steht Impfungen skeptisch gegenüber.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Datenverarbeitung ---\n",
        "def parse_entry(entry: dict) -> dict:\n",
        "    \"\"\"Verarbeitet Rohdaten zu einem strukturierten Prompt\"\"\"\n",
        "    text = entry[\"text\"]\n",
        "\n",
        "    # Extrahiere Key-Value Paare\n",
        "    persona_data = {}\n",
        "    for pair in re.split(r'\\nAussage: ', text):\n",
        "        if match := re.match(r'^\"(.*?)\" → Antwort: \"(.*?)\"', pair):\n",
        "            key = match.group(1).replace(\"Aussage: \", \"\").strip()\n",
        "            persona_data[key] = match.group(2).strip()\n",
        "\n",
        "    # Validiere erforderliche Felder\n",
        "    required_fields = ['GENDER', 'AGE_CAT', 'EDU_CAT', 'BULA', 'Impfen: Ehestmöglich']\n",
        "    for field in required_fields:\n",
        "        if field not in persona_data:\n",
        "            persona_data[field] = 'Unbekannt'\n",
        "\n",
        "    # Erstelle Prompt\n",
        "    prompt = f\"\"\"Person:\n",
        "- Geschlecht: {persona_data['GENDER']}\n",
        "- Alter: {persona_data['AGE_CAT']}\n",
        "- Bildung: {persona_data['EDU_CAT']}\n",
        "- Bundesland: {persona_data['BULA']}\n",
        "- Impfen Ehestmöglich: {persona_data['Impfen: Ehestmöglich']}\"\"\"\n",
        "\n",
        "    return {\"prompt\": prompt, \"response\": \"\"}\n",
        "\n",
        "def create_prompt(example: dict) -> str:\n",
        "    \"\"\"Erstellt den finalen Prompt mit Few-Shot Beispielen\"\"\"\n",
        "    system_prompt = \"\"\"Erstelle eine Personenbeschreibung in einem Satz nach diesen Regeln:\n",
        "1. Beginne mit dem Alter (z.B. \"30-39-jährige\")\n",
        "2. Verwende nur die angegebenen Informationen\n",
        "3. Struktur: [Alter] [Bildung] [Geschlecht] aus [Bundesland] [Impfverhalten]\"\"\"\n",
        "\n",
        "    example_template = \"\"\"\\\n",
        "Beispiel-Eingabe:\n",
        "{input}\n",
        "\n",
        "Beispiel-Ausgabe:\n",
        "{output}\"\"\"\n",
        "\n",
        "    examples = \"\\n\\n\".join(\n",
        "        example_template.format(**ex)\n",
        "        for ex in FEW_SHOT_EXAMPLES\n",
        "    )\n",
        "\n",
        "    return f\"\"\"\\\n",
        "{system_prompt}\n",
        "\n",
        "{examples}\n",
        "\n",
        "Eingabe:\n",
        "{example['prompt']}\n",
        "\n",
        "Ausgabe:\"\"\"\n",
        "\n",
        "# --- Modelloperationen ---\n",
        "def initialize_model() -> pipeline:\n",
        "    \"\"\"Initialisiert das Modell und den Tokenizer\"\"\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=torch.float32\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=DEVICE\n",
        "    )\n",
        "\n",
        "def clean_response(text: str) -> str:\n",
        "    \"\"\"Bereinigt die generierte Antwort\"\"\"\n",
        "    # Entferne unerwünschte Muster\n",
        "    text = re.sub(r'<\\|.*?\\|>|<\\/s>|\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'\\b\\d+\\+?\\b', '', text)\n",
        "\n",
        "    # Erzwinge korrekte Formatierung\n",
        "    if match := re.search(r'(\\d+-\\d+)\\s*Jahre', text):\n",
        "        text = text.replace(match.group(0), f\"{match.group(1)}-jährige\")\n",
        "\n",
        "    return text.strip(\". \\n\")\n",
        "\n",
        "# --- Hauptprozess ---\n",
        "def main():\n",
        "    # 1. Daten vorbereiten\n",
        "    raw_data = [{\"prompt\": entry} for entry in dataset]  # Ihre Rohdaten\n",
        "    dataset = Dataset.from_dict({\"prompt\": raw_data})\n",
        "    dataset = dataset.map(parse_entry)\n",
        "\n",
        "    # 2. Modell initialisieren\n",
        "    pipe = initialize_model()\n",
        "\n",
        "    # 3. Generierung durchführen\n",
        "    def generate(example):\n",
        "        prompt = create_prompt(example)\n",
        "        output = pipe(prompt, **GENERATION_CONFIG)[0]['generated_text']\n",
        "        response = output.split(\"Ausgabe:\")[-1].split(\"\\n\")[0].strip()\n",
        "        example[\"response\"] = clean_response(response)\n",
        "        return example\n",
        "\n",
        "    dataset = dataset.map(generate)\n",
        "\n",
        "    # 4. Ergebnisse anzeigen\n",
        "    for idx in range(3):\n",
        "        print(\"\\nPrompt:\")\n",
        "        print(dataset[idx][\"prompt\"])\n",
        "        print(\"\\nGenerierte Antwort:\")\n",
        "        print(dataset[idx][\"response\"])\n",
        "        print(\"=\"*50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "kuJ-SJTxCopF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "from datasets import Dataset\n",
        "\n",
        "# Few-shot examples\n",
        "few_shot_examples = [\n",
        "    {\n",
        "        \"user\": \"\"\"Analysiere diese Person:\n",
        "Geschlecht: Weiblich\n",
        "Alter: 30-39 Jahre\n",
        "Bildung: Hochschulabschluss\n",
        "Wohnort: Wien\n",
        "Impfbereitschaft: Hoch\"\"\",\n",
        "        \"assistant\": \"\"\"Die 30-39-jährige Akademikerin aus Wien zeigt eine hohe Bereitschaft zur zeitnahen Impfung.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"user\": \"\"\"Analysiere diese Person:\n",
        "Geschlecht: Männlich\n",
        "Alter: 60-69 Jahre\n",
        "Bildung: Berufsausbildung\n",
        "Wohnort: Tirol\n",
        "Impfbereitschaft: Gering\"\"\",\n",
        "        \"assistant\": \"\"\"Der 60-69-jährige Handwerker aus Tirol steht Impfungen kritisch gegenüber.\"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Select relevant few-shot examples\n",
        "def select_relevant_examples(prompt):\n",
        "    age_group_match = re.search(r\"Alter: (.+?) Jahre\", prompt)\n",
        "    if age_group_match:\n",
        "        age_group = age_group_match.group(1)\n",
        "        relevant_examples = [ex for ex in few_shot_examples if age_group in ex[\"user\"]]\n",
        "        if relevant_examples:\n",
        "            return [random.choice(relevant_examples)]\n",
        "    return [random.choice(few_shot_examples)]\n",
        "\n",
        "# Format into chat messages\n",
        "def format_chat_template(example, tokenizer=None):\n",
        "    system_prompt = \"\"\"Erstelle präzise Personenbeschreibungen in einem Satz nach diesen Regeln:\n",
        "1. Beginne immer mit dem Alter (z.B. \"30-39-jährige\")\n",
        "2. Verwende nur die gegebenen Attribute\n",
        "3. Struktur: [Alter] [Bildung] [Geschlecht] aus [Ort] [Impfverhalten]\n",
        "4. Keine Zusatzinformationen erfinden\n",
        "\n",
        "Beispiel: \"Die 30-39-jährige Hochschulabsolventin aus Wien befürwortet eine rasche Impfung.\"\n",
        "\"\"\"\n",
        "\n",
        "    selected_examples = select_relevant_examples(example[\"prompt\"])\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "    for ex in selected_examples:\n",
        "        messages.extend([\n",
        "            {\"role\": \"user\", \"content\": f\"Analysiere:\\n{ex['user']}\"},\n",
        "            {\"role\": \"assistant\", \"content\": f\"{ex['assistant']}\"}\n",
        "        ])\n",
        "\n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Analysiere:\\n{example['prompt']}\\n\\nZusammenfassung:\"\n",
        "    })\n",
        "\n",
        "    # Manual formatting of chat if tokenizer.apply_chat_template fails\n",
        "    formatted_messages = \"\"\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"system\":\n",
        "            formatted_messages += f\"<|system|>\\n{message['content']}\\n\"\n",
        "        elif message[\"role\"] == \"user\":\n",
        "            formatted_messages += f\"<|user|>\\n{message['content']}\\n\"\n",
        "        elif message[\"role\"] == \"assistant\":\n",
        "            formatted_messages += f\"<|assistant|>\\n{message['content']}\\n\"\n",
        "    formatted_messages += \"<|assistant|>\\n\"\n",
        "\n",
        "    example[\"text\"] = formatted_messages\n",
        "    return example\n",
        "\n",
        "# Pretty-print helper\n",
        "def pretty_print_series_row(dataset, row_number):\n",
        "    if not isinstance(dataset, Dataset):\n",
        "        raise ValueError(\"Input must be a Hugging Face Dataset\")\n",
        "    row = dataset[row_number]\n",
        "    text = row.get('text', '')\n",
        "    formatted = text.replace('\\\\n', '\\n').replace('- ', '  - ')\n",
        "    print(formatted)\n",
        "\n",
        "# Clean response\n",
        "def clean_response(response):\n",
        "    response = re.sub(r'\\d+[.,]\\d+|\\d+-\\d+|\\d+[/]\\d+', '', response)\n",
        "    response = re.sub(r'\\b(Krankenhaus|Medikamente|Survey)\\b', '', response, flags=re.I)\n",
        "    if re.search(r'\\d+-\\d+', response) and 'jährig' not in response:\n",
        "        age_match = re.search(r'(\\d+-\\d+)', response)\n",
        "        response = f\"{age_match.group(1)}-jährige \" + response.replace(age_match.group(1), '')\n",
        "    return response\n",
        "\n",
        "# Generate persona\n",
        "def generate_persona(pipe, text, max_new_tokens=60):\n",
        "    base_info = text.split('Person:')[-1].split('<|assistant|>')[0].strip()\n",
        "    prompt = f\"\"\"Erstelle einen präzisen Satz nach diesem Muster:\n",
        "Angaben: Geschlecht: Weiblich, Alter: 30-39, Bildung: Hoch, Bundesland: Wien, Impfen: Trifft eher zu\n",
        "Satz: Die 30-39-jährige hochgebildete Frau aus Wien tendiert zu einer baldigen Impfung.\n",
        "\n",
        "Verwende diese Angaben:\n",
        "{base_info}\n",
        "Satz:\"\"\"\n",
        "\n",
        "    outputs = pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        top_k=10,\n",
        "        top_p=0.60,\n",
        "        repetition_penalty=1.2,\n",
        "        no_repeat_ngram_size=2,\n",
        "        num_beams=5\n",
        "    )\n",
        "\n",
        "    raw_response = outputs[0]['generated_text'].split(\"Zusammenfassung:\")[-1].strip()\n",
        "    response = clean_response(raw_response)\n",
        "\n",
        "    print(\"Prompt:\\n\", prompt)\n",
        "    print(\"\\nAntwort:\\n\", response)\n",
        "    return response\n",
        "\n",
        "# Example Usage:\n",
        "dataset = dataset.map(lambda example: format_chat_template(example))\n",
        "# pretty_print_series_row(dataset, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "ffzHMfVG_Sn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# 1. Load a small model pipeline for generation\n",
        "pipe = pipeline(\"text-generation\", model=\"gpt2\")  # or use your own model\n",
        "\n",
        "# 2. Example: create your dataset manually\n",
        "data = {\n",
        "    \"prompt\": [\n",
        "        \"\"\"Geschlecht: Weiblich\n",
        "Alter: 30-39 Jahre\n",
        "Bildung: Hochschulabschluss\n",
        "Wohnort: Wien\n",
        "Impfbereitschaft: Hoch\"\"\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_dict(data)\n",
        "\n",
        "# 3. Format your dataset into chat template\n",
        "dataset = dataset.map(lambda example: format_chat_template(example))\n",
        "\n",
        "# 4. Pick the row you want\n",
        "row_number = 0\n",
        "row = dataset[row_number]\n",
        "\n",
        "# 5. Get the text\n",
        "text = row['text']\n",
        "\n",
        "# 6. Generate the persona sentence\n",
        "persona = generate_persona(pipe, text)\n",
        "\n",
        "# 7. Print both\n",
        "print(\"\\n\\nFinal Output:\\n\")\n",
        "print(\"Prompt:\")\n",
        "print(text)\n",
        "print(\"\\nGenerated Persona:\")\n",
        "print(persona)"
      ],
      "metadata": {
        "id": "l4j_1BdiALJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- Konfiguration ---\n",
        "MODEL_NAME = \"dbmdz/german-gpt2\"\n",
        "GENERATION_CONFIG = {\n",
        "    \"max_new_tokens\": 60,\n",
        "    \"do_sample\": True,\n",
        "    \"temperature\": 0.4,\n",
        "    \"top_k\": 30,\n",
        "    \"top_p\": 0.85,\n",
        "    \"repetition_penalty\": 1.5,\n",
        "    \"no_repeat_ngram_size\": 2,\n",
        "    \"num_beams\": 2\n",
        "}\n",
        "\n",
        "FEW_SHOT_EXAMPLES = [\n",
        "    {\n",
        "        \"input\": \"\"\"Person:\n",
        "- Geschlecht: Weiblich\n",
        "- Alter: 30-39 Jahre\n",
        "- Bildung: Hoch\n",
        "- Bundesland: Wien\n",
        "- Impfen Ehestmöglich: Trifft eher zu\"\"\",\n",
        "        \"output\": \"\"\"Die 30-39-jährige hochgebildete Frau aus Wien tendiert zu einer baldigen Impfung.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"\"\"Person:\n",
        "- Geschlecht: Männlich\n",
        "- Alter: 60-69 Jahre\n",
        "- Bildung: Mittel\n",
        "- Bundesland: Tirol\n",
        "- Impfen Ehestmöglich: Trifft eher nicht zu\"\"\",\n",
        "        \"output\": \"\"\"Der 60-69-jährige Mann mit mittlerer Bildung aus Tirol steht einer zeitnahen Impfung skeptisch gegenüber.\"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- Hilfsfunktionen ---\n",
        "def clean_response(text: str) -> str:\n",
        "    \"\"\"Bereinigt die generierte Antwort von unerwünschten Inhalten\"\"\"\n",
        "    # Entferne unerwünschte Muster\n",
        "    patterns_to_remove = [\n",
        "        r'<\\|.*?\\|>',  # Chat-Tags\n",
        "        r'\\d+[.,]\\d+|\\d+-\\d+|\\d+[/]\\d+',  # Zahlenformate\n",
        "        r'\\b(Krankenhaus|Medikamente|Survey)\\b'  # Unerwünschte Begriffe\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns_to_remove:\n",
        "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Normalisiere Leerzeichen\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Erzwinge korrekte Altersformatierung\n",
        "    if age_match := re.search(r'(\\d+-\\d+)', text):\n",
        "        if 'jährig' not in text:\n",
        "            text = f\"{age_match.group(1)}-jährige \" + text.replace(age_match.group(1), '')\n",
        "\n",
        "    # Erzwinge Satzende\n",
        "    if not text.endswith(('.', '!', '?')):\n",
        "        text = f\"{text.rstrip('.')}.\"\n",
        "\n",
        "    return text\n",
        "\n",
        "def select_relevant_example(prompt: str) -> dict:\n",
        "    \"\"\"Wählt ein relevantes Few-Shot-Beispiel basierend auf dem Alter aus\"\"\"\n",
        "    if age_match := re.search(r\"Alter: (.+?) Jahre\", prompt):\n",
        "        age_group = age_match.group(1)\n",
        "        return next((ex for ex in FEW_SHOT_EXAMPLES if age_group in ex[\"input\"]), None)\n",
        "    return random.choice(FEW_SHOT_EXAMPLES)\n",
        "\n",
        "# --- Hauptfunktionen ---\n",
        "def format_chat_template(example: dict, tokenizer: AutoTokenizer) -> dict:\n",
        "    \"\"\"Formatiert den Beispieltext in ein Chat-Template\"\"\"\n",
        "    system_prompt = \"\"\"Erstellen Sie eine prägnante Personenbeschreibung in einem Satz:\n",
        "- Beginne mit dem Alter (z.B. \"30-39-jährige\")\n",
        "- Verwende nur die angegebenen Attribute\n",
        "- Struktur: [Alter] [Bildung] [Geschlecht] aus [Ort] [Impfverhalten]\n",
        "- Keine zusätzlichen Informationen erfinden\"\"\"\n",
        "\n",
        "    example_data = example[\"prompt\"]\n",
        "    selected_example = select_relevant_example(example_data)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": selected_example[\"input\"]},\n",
        "        {\"role\": \"assistant\", \"content\": selected_example[\"output\"]},\n",
        "        {\"role\": \"user\", \"content\": example_data}\n",
        "    ]\n",
        "\n",
        "    example[\"text\"] = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    return example\n",
        "\n",
        "def generate_persona(pipe: pipeline, text: str) -> str:\n",
        "    \"\"\"Generiert eine Personenbeschreibung basierend auf den Eingabedaten\"\"\"\n",
        "    base_info = re.search(r\"Person:(.*?)<\\|assistant\\|>\", text, re.DOTALL).group(1).strip()\n",
        "\n",
        "    prompt = f\"\"\"Beispiel:\n",
        "Input: {FEW_SHOT_EXAMPLES[0]['input']}\n",
        "Output: {FEW_SHOT_EXAMPLES[0]['output']}\n",
        "\n",
        "Generiere für folgende Daten:\n",
        "{base_info}\n",
        "Output:\"\"\"\n",
        "\n",
        "    output = pipe(prompt, **GENERATION_CONFIG)[0]['generated_text']\n",
        "    response = output.split(\"Output:\")[-1].strip()\n",
        "    return clean_response(response)\n",
        "\n",
        "# --- Initialisierung ---\n",
        "def initialize_pipeline() -> pipeline:\n",
        "    \"\"\"Initialisiert die Model-Pipeline\"\"\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float32\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        model_kwargs={\"pad_token_id\": tokenizer.eos_token_id}\n",
        "    )\n",
        "\n",
        "# --- Hauptablauf ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Annahme: dataset ist bereits definiert\n",
        "    pipe = initialize_pipeline()\n",
        "    dataset = dataset.map(lambda ex: format_chat_template(ex, pipe.tokenizer))\n",
        "\n",
        "    new_texts = []\n",
        "    for entry in dataset:\n",
        "        response = generate_persona(pipe, entry[\"text\"])\n",
        "        updated_text = entry[\"text\"].replace(\"<|assistant|>\", f\"<|assistant|>\\n{response}</s>\")\n",
        "        new_texts.append(updated_text)\n",
        "\n",
        "    dataset = dataset.add_column(\"text_with_response\", new_texts)"
      ],
      "metadata": {
        "id": "5MdZWGQQ-1vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_examples = [\n",
        "    {\n",
        "        \"user\": \"\"\"Analysiere diese Person:\n",
        "Geschlecht: Weiblich\n",
        "Alter: 30-39 Jahre\n",
        "Bildung: Hochschulabschluss\n",
        "Wohnort: Wien\n",
        "Impfbereitschaft: Hoch\"\"\",\n",
        "        \"assistant\": \"\"\"Zusammenfassung: Die 30-39-jährige Akademikerin aus Wien zeigt eine hohe Bereitschaft zur zeitnahen Impfung.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"user\": \"\"\"Analysiere diese Person:\n",
        "Geschlecht: Männlich\n",
        "Alter: 60-69 Jahre\n",
        "Bildung: Berufsausbildung\n",
        "Wohnort: Tirol\n",
        "Impfbereitschaft: Gering\"\"\",\n",
        "        \"assistant\": \"\"\"Zusammenfassung: Der 60-69-jährige Handwerker aus Tirol steht Impfungen kritisch gegenüber.\"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "def select_relevant_examples(prompt):\n",
        "    age_group_match = re.search(r\"Alter: (.+?) Jahre\", prompt)\n",
        "    if age_group_match:\n",
        "        age_group = age_group_match.group(1)\n",
        "        relevant_examples = [ex for ex in few_shot_examples if age_group in ex[\"user\"]]\n",
        "        if relevant_examples:\n",
        "            return [random.choice(relevant_examples)]\n",
        "    return [random.choice(few_shot_examples)]\n",
        "\n",
        "def format_chat_template(example, tokenizer):\n",
        "\n",
        "\n",
        "    system_prompt = \"\"\"Erstelle präzise Personenbeschreibungen in einem Satz nach diesen Regeln:\n",
        "1. Beginne immer mit dem Alter (z.B. \"30-39-jährige\")\n",
        "2. Verwende nur die gegebenen Attribute\n",
        "3. Struktur: [Alter] [Bildung] [Geschlecht] aus [Ort] [Impfverhalten]\n",
        "4. Keine Zusatzinformationen erfinden\n",
        "\n",
        "Beispielantwort: \"Die 30-39-jährige Hochschulabsolventin aus Wien befürwortet eine rasche Impfung.\"\n",
        "\"\"\"\n",
        "\n",
        "    selected_examples = select_relevant_examples(example[\"prompt\"])\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
        "\n",
        "    for ex in selected_examples:\n",
        "        messages.extend([\n",
        "            {\"role\": \"user\", \"content\": f\"Analysiere:\\n{ex['user']}\"},\n",
        "            {\"role\": \"assistant\", \"content\": f\"Zusammenfassung: {ex['assistant']}\"}\n",
        "        ])\n",
        "\n",
        "    messages.append({\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"Analysiere:\\n{example['prompt']}\\n\\nZusammenfassung:\"\n",
        "    })\n",
        "\n",
        "    example[\"text\"] = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    return example"
      ],
      "metadata": {
        "id": "TSll0B2Yhsgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turn dataset into chat format\n",
        "dataset = dataset.map(lambda example: format_chat_template(example, tokenizer))\n"
      ],
      "metadata": {
        "id": "qoUQxwGhtjVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this funciton just serves as a way to check the input of the LLM\n",
        "def pretty_print_series_row(dataset, row_number):\n",
        "    # ensure the input is a Hugging Face Dataset\n",
        "    if not isinstance(dataset, Dataset):\n",
        "        raise ValueError(\"Input must be a Hugging Face Dataset\")\n",
        "\n",
        "    # access the row using the row_number\n",
        "    row = dataset[row_number]\n",
        "\n",
        "    # assuming the row is a dictionary-like object\n",
        "    text = row.get('text', '')  # replace 'text' with the actual column name containing the text\n",
        "\n",
        "    # format the text, to improve readability\n",
        "    formatted = (text\n",
        "                 .replace('\\\\n', '\\n')\n",
        "                 .replace('- ', '  - ')\n",
        "                )\n",
        "    print(formatted)\n",
        "\n",
        "pretty_print_series_row(dataset, row_number = 1)"
      ],
      "metadata": {
        "id": "bxVuVeuytp_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a clean response\n",
        "def clean_response(response):\n",
        "    # exclude medical information\n",
        "    response = re.sub(r'\\d+[.,]\\d+|\\d+-\\d+|\\d+[/]\\d+', '', response)\n",
        "    response = re.sub(r'\\b(Krankenhaus|Medikamente|Survey)\\b', '', response, flags=re.I)\n",
        "    # force correct\n",
        "    if re.search(r'\\d+-\\d+', response) and 'jährig' not in response:\n",
        "        age_match = re.search(r'(\\d+-\\d+)', response)\n",
        "        response = f\"{age_match.group(1)}-jährige \" + response.replace(age_match.group(1), '')\n",
        "    return response\n",
        "\n",
        "# generate the persona (turn input into a single sentence)\n",
        "def generate_persona(pipe, text, max_new_tokens=60):\n",
        "    base_info = text.split('Person:')[-1].split('<|assistant|>')[0].strip()\n",
        "    prompt = f\"\"\"Erstelle einen präzisen Satz nach diesem Muster:\n",
        "        Angaben: Geschlecht: Weiblich, Alter: 30-39, Bildung: Hoch, Bundesland: Wien, Impfen: Trifft eher zu\n",
        "        Satz: Die 30-39-jährige hochgebildete Frau aus Wien tendiert zu einer baldigen Impfung.\n",
        "\n",
        "        Verwende diese Angaben:\n",
        "        {base_info}\n",
        "        Satz:\"\"\"\n",
        "\n",
        "    outputs = pipe(\n",
        "        prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True, # sample the next word from probability distribution\n",
        "        temperature=0.2, # introduce \"playfullness\", lower = sharper, model does more strictly what it was advised to do\n",
        "        top_k=10, # consider only the top 40 words as the next word\n",
        "        top_p=0.60, # choose one of the words, which compromise 90% of the probability\n",
        "        repetition_penalty=1.2,# this parameter aims to prevent the model from repeating itself. 1 no penalty, value > 1 increase the penalty, making the model less likely to repeat\n",
        "        no_repeat_ngram_size=2, # do not only use 2 words in the output\n",
        "        num_beams=5  # create two text-variants and choose the best one\n",
        "    )\n",
        "\n",
        "    raw_response = outputs[0]['generated_text'].split(\"Zusammenfassung:\")[-1].strip()\n",
        "\n",
        "    # Bereinige die Antwort\n",
        "    response = clean_response(raw_response)\n",
        "    print(\"Prompt:\\n\", prompt)\n",
        "    print(\"\\nAntwort:\\n\", response)\n",
        "    return response"
      ],
      "metadata": {
        "id": "2eadFcpgm-Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model and tokenizer\n",
        "model_name = \"dbmdz/german-gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float32\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "Zqs1_drMn6AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate pipeline\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    model_kwargs={\n",
        "        \"pad_token_id\": tokenizer.eos_token_id\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "bgcQ8unzn8Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate personas:\n",
        "new_texts = []\n",
        "for idx in range(len(dataset)):\n",
        "    response = generate_persona(pipe, dataset[idx][\"text\"])\n",
        "    updated_text = dataset[idx][\"text\"].replace(\"<|assistant|>\", f\"<|assistant|>\\n{response}</s>\")\n",
        "    new_texts.append(updated_text)\n",
        "\n",
        "# create new dataset with generated personas\n",
        "# dataset = dataset.add_column(\"text_with_response\", new_texts)"
      ],
      "metadata": {
        "id": "NJdJrDJ5tq9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Delete?"
      ],
      "metadata": {
        "id": "FyytoTc7uiaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Remove GPU-specific configs\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./cpu-finetuned\",\n",
        "    num_train_epochs=1,  # Start with 1 epoch\n",
        "    per_device_train_batch_size=1,  # Batch size 1 for CPU\n",
        "    learning_rate=1e-5,  # Lower learning rate\n",
        "    logging_steps=5,\n",
        "    save_strategy=\"no\",  # Disable saving to save RAM\n",
        "    report_to=\"none\",\n",
        "    use_cpu=True  # Force CPU usage\n",
        ")\n",
        "\n",
        "# Simplified LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Smaller rank\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "d8CGmvc_-hJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "prompt = \"Write a Python function that can clean the HTML tags from the file:\"\n",
        "\n",
        "outputs = pipe(\n",
        "    prompt,\n",
        "    max_new_tokens=300,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        ")\n",
        "print(outputs[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "crLmaBDJ-u_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jrKaDDH_U_ld"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}