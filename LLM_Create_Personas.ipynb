{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNEIY9PAKV+dPuw3rmhK1xF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "79006067b7234ba298489a7f82b924a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_72925d2d7265423391a9816a77390354"
          }
        },
        "8d723c105c7e4cdca80b7c354e1157fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a4b8a997d2a4fea93f0bf32af65d99c",
            "placeholder": "​",
            "style": "IPY_MODEL_07038ec358eb4561a31bb803b81093cc",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "a6fa6467f7e4403e837dcbfa4813157c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_d3aaaa70ba3a4db19114a60499601049",
            "placeholder": "​",
            "style": "IPY_MODEL_c07fa4a998e04411a91ecfb172e76a12",
            "value": ""
          }
        },
        "1fec4729d8924a25b930ad83c7224fd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_ba502df6f4ee4d66a05b67b7153f6461",
            "style": "IPY_MODEL_3e9a0df988dd44e6a606d44709df1e10",
            "value": true
          }
        },
        "33dd6349c28541a887780310aaacba92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_ca7e3482c4a647abb0cbc6c3fde304cc",
            "style": "IPY_MODEL_50de3d3692a949d5925da25aeb6117e6",
            "tooltip": ""
          }
        },
        "d2a7868813b54309869689c5eb1e10e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d624ca8663741e88dcf1904d9306477",
            "placeholder": "​",
            "style": "IPY_MODEL_b96f09f7228145279a4dce49aa9a59ee",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "72925d2d7265423391a9816a77390354": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "6a4b8a997d2a4fea93f0bf32af65d99c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07038ec358eb4561a31bb803b81093cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3aaaa70ba3a4db19114a60499601049": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c07fa4a998e04411a91ecfb172e76a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba502df6f4ee4d66a05b67b7153f6461": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e9a0df988dd44e6a606d44709df1e10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca7e3482c4a647abb0cbc6c3fde304cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50de3d3692a949d5925da25aeb6117e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "9d624ca8663741e88dcf1904d9306477": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b96f09f7228145279a4dce49aa9a59ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd89c4a4a5f244df84da75c1eeec97a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_232359131bb54d53ad5b11564f65cb09",
            "placeholder": "​",
            "style": "IPY_MODEL_511882d0d4704d8298f7ee0ddeeda124",
            "value": "Connecting..."
          }
        },
        "232359131bb54d53ad5b11564f65cb09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "511882d0d4704d8298f7ee0ddeeda124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LorenzEh/LLM-Vaccine-Hesitency/blob/main/LLM_Create_Personas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the libraries\n"
      ],
      "metadata": {
        "id": "H30iVz4gctjH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjEOHn3qu9gm"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets peft accelerate bitsandbytes trl llama-cpp-python huggingface-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the data ready"
      ],
      "metadata": {
        "id": "d5taHxxHR_j8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "m0Fd1kFava5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download(id=\"1_n3zhxBFG0QPabe2cKeq_UY092cGDW4V\", output=\"data_imputed_decoded.csv\", quiet=False) # unfortunatly the csv does not work, therefore we need to continue with the stata format\n",
        "data = pd.read_csv(\"data_imputed_decoded.csv\") # copy and paste a single"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kctU23lHv8MQ",
        "outputId": "02455c7d-3b8e-4638-ff91-539a4d622702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_n3zhxBFG0QPabe2cKeq_UY092cGDW4V\n",
            "To: /content/data_imputed_decoded.csv\n",
            "100%|██████████| 1.00M/1.00M [00:00<00:00, 45.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['EDU_CAT'] = data['EDU_CAT'].str.replace(r' \\(Kat\\. \\d+(-\\d+)?\\)', '', regex=True) # delete the \"(Kat )\" in EDU_CAT"
      ],
      "metadata": {
        "id": "18QkvAjBqHPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in data.columns: # get a list of all columns\n",
        "    print(col)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTPDDhfIw84N",
        "outputId": "0dc20e33-68a9-457a-f51a-4a18be7245fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aussage: Akzeptabel höhere Steuern zu zahlen\n",
            "Aussage: Lebensweise ändern\n",
            "Aussage: Wichtigeres als Corona im Leben\n",
            "Aussage: Behauptungen über Corona übertrieben\n",
            "Aussage: Solange andere Verhalten nicht ändern keine Verhaltensänderung\n",
            "Zufriedenheit Bundesregierung: Bilanz\n",
            "Zufriedenheit Bundesregierung: Bekämpfung der Arbeitslosigkeit\n",
            "Zufriedenheit Bundesregierung: Stützung der kleinen und mittleren Unternehmen\n",
            "Zufriedenheit Bundesregierung: Stützung der Wirtschaft allgemein\n",
            "Aussage: Beitrag zur Finanzierung durch alle Personen gleich\n",
            "Aussage: Kluft zwischen Arm und Reich wird durch Krise größer\n",
            "Aussage: Neue Steuern schaden Wirtschaftsstandort Österreich\n",
            "Aussage: Politik muss soziale Ungleichheit bekämpfen\n",
            "Aussage: Sozialstaat macht Menschen träge und faul\n",
            "Aussage: Arbeitslosigkeit durch Staatsschulden bekämpfen\n",
            "Aussage: Wenig Eingriff in Wirtschaft durch Politik \n",
            "Aussage: Bevorzugung von Frauen bei gleicher Qualifikation \n",
            "Aussage: Ausweitung der Befugnisse der Polizei\n",
            "Aussage: Zuwanderung nach Österreich nur in Ausnahmefällen\n",
            "Aussage: Umweltschutz um jeden Preis\n",
            "Aussage: Einkommensunterschiede in Österreich sind zu groß\n",
            "Aussage: Generationenvertrag\n",
            "Aussage: Reduktion von Einkommensunterschieden durch staatliche Maßnahmen\n",
            "Aussage: Maßnahmen zur Eindämmung des Coronavirus waren Fehler\n",
            "Aussage: Regierung übertreibt hinsichtlich gesundheitlicher Gefahr\n",
            "Impfen: Ehestmöglich \n",
            "Impfen: Impfpflicht für alle\n",
            "Impfen: Gratis Impfstoff \n",
            "Aussage: Das Coronavirus ist gefährlicher als eine normale Grippe\n",
            "Aussage: Arbeitslose suchen nicht nach Arbeitsstelle\n",
            "Aussage: Leute mit geringem Einkommen erhalten weniger Sozialleistungen\n",
            "Aussage: Es gelingt Leuten, Sozialleistungen zu erhalten, ohne Anspruch\n",
            "Aussage: Mache mir Sorgen über unvorhergesehene Nebenwirkungen der Impfung\n",
            "Aussage: Verlasse mich lieber auf mein Immunsystem als auf eine Impfung\n",
            "Aussage: Fühle mich gut über die Wirkweise der Impfstoffe informiert\n",
            "Aussage: Die Impfung erlaubt mir ein Leben wie vor der Pandemie\n",
            "Aussage: Mit einer Impfung kann ich andere schützen\n",
            "Aussage: Lasse mich impfen, wenn andere sich zuerst impfen lassen\n",
            "Aussage: Mit einer Impfung kann ich mich selbst schützen\n",
            "Aussage: Behördlich zugelassene Impfungen sind sicher\n",
            "Aussage: Behörden informieren hinreichend über Wirkungsweise der Impfstoffe\n",
            "Aussage: Mein Alltag ist zu stressig, ich habe keine Zeit für eine Impfung\n",
            "Impfen: Impfpflicht für Berufsgruppen mit hohem Risiko\n",
            "Aussage: Alles in allem empfinde ich die Corona-Maßnahmen als gerecht\n",
            "Impfen: Meisten Personen in meinem Umfeld sind bereit sich impfen zu lassen\n",
            "GENDER\n",
            "AGE_CAT\n",
            "EDU_CAT\n",
            "BULA\n",
            "Depressivität: Ruhig und gelassen\n",
            "Depressivität: Einsam\n",
            "Depressivität: Ärgerlich\n",
            "Depressivität: So niedergeschlagen\n",
            "Depressivität: Glücklich\n",
            "Depressivität: Sehr nervös\n",
            "Depressivität: Ängstlich\n",
            "Depressivität: Bedrückt und traurig\n",
            "Depressivität: Voller Energie\n",
            "Zufriedenheit: Familienleben\n",
            "Zufriedenheit: Partnerschaft\n",
            "Zufriedenheit: Beruf\n",
            "Zufriedenheit: Freizeit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fixing small issue in variable names:\n",
        "data.columns = data.columns.str.strip()"
      ],
      "metadata": {
        "id": "wohNdXhFEHgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert tabular data into sentences\n",
        "\n",
        "\n",
        "1.   **Develop well-structured prompts from the tabular data:** This involves carefully crafting the input to guide the Large Language Model (LLM) effectively.\n",
        "2.   **Implement a chat template for enhanced model comprehension:** Introducing a consistent chat template significantly improves the model's\n",
        "understanding of the input, particularly crucial for smaller, less powerful models running on Colab's CPU. Furthermore, the prompt engineering process includes creating illustrative examples and establishing a clear structural framework within the prompts.\n",
        "3.   **Adjust parameters for output optimization:** This involves fine-tuning parameters such as temperature, repeat_penalty, and top_k to achieve the desired quality and characteristics of the generated text.\n",
        "\n",
        "Considering the model's lightweight architecture, the current results are reasonably satisfactory for most generated personas. A larger model would significantly improve outcomes, reduce reliance on extensive prompt engineering, and enable the inclusion of substantially more variables, leading to much richer and more detailed persona profiles. Also, much more distinct personas could be generated."
      ],
      "metadata": {
        "id": "RgplYSKSdFGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# set random seed for reproducibility\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rziXEnTjewQ3",
        "outputId": "0c3c804d-d9d8-40b8-becd-a93927309812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7bb9a6226630>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompts_directly(df):\n",
        "    data_dict = {\"prompt\": [], \"response\": []}\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        # Build prompt directly from row data\n",
        "        prompt = f\"\"\"Person:\n",
        "- Geschlecht: {row.get('GENDER', 'Unbekannt')}\n",
        "- Alter: {row.get('AGE_CAT', 'Unbekannt')}\n",
        "- Bildung: {row.get('EDU_CAT', 'Unbekannt')}\n",
        "- Bundesland: {row.get('BULA', 'Unbekannt')}\n",
        "- Impfen Ehestmöglich: {row.get('Impfen: Ehestmöglich', 'Unbekannt')}\n",
        "- Verlasse mich lieber auf mein Immunsystem: {row.get('Aussage: Verlasse mich lieber auf mein Immunsystem als auf eine Impfung', 'Unbekannt')}\n",
        "- Selbstschutz durch Impfung: {row.get('Aussage: Mit einer Impfung kann ich mich selbst schützen', 'Unbekannt')}\n",
        "- Fremdschutz durch Impfung: {row.get('Aussage: Mit einer Impfung kann ich andere schützen', 'Unbekannt')}\n",
        "- Behördeninformation: {row.get('Aussage: Behörden informieren hinreichend über Wirkungsweise der Impfstoffe', 'Unbekannt')}\"\"\"\n",
        "\n",
        "        data_dict[\"prompt\"].append(prompt)\n",
        "        data_dict[\"response\"].append(\"[PLACEHOLDER_FOR_GENERATED_PERSONA]\")\n",
        "\n",
        "    return Dataset.from_dict(data_dict)\n",
        "dataset = create_prompts_directly(data).select(range(50))"
      ],
      "metadata": {
        "id": "czzVInb744y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(dataset[\"prompt\"][i]) # description of person which is used in the prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6HwOCxR5CL1",
        "outputId": "853b8105-a83b-479f-f771-ecf92bc2f1c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Person:\n",
            "- Geschlecht: Maennlich\n",
            "- Alter: 65+ Jahre\n",
            "- Bildung: Hoch\n",
            "- Bundesland: Vorarlberg\n",
            "- Impfen Ehestmöglich: Trifft eher nicht zu\n",
            "- Verlasse mich lieber auf mein Immunsystem: Trifft gar nicht zu\n",
            "- Selbstschutz durch Impfung: Teils-teils\n",
            "- Fremdschutz durch Impfung: Teils-teils\n",
            "- Behördeninformation: Teils-teils\n",
            "Person:\n",
            "- Geschlecht: Weiblich\n",
            "- Alter: 50-64 Jahre\n",
            "- Bildung: Niedrig\n",
            "- Bundesland: Oberoesterreich\n",
            "- Impfen Ehestmöglich: Trifft eher zu\n",
            "- Verlasse mich lieber auf mein Immunsystem: Trifft gar nicht zu\n",
            "- Selbstschutz durch Impfung: Trifft eher zu\n",
            "- Fremdschutz durch Impfung: Trifft eher zu\n",
            "- Behördeninformation: Trifft eher zu\n",
            "Person:\n",
            "- Geschlecht: Maennlich\n",
            "- Alter: 40-49 Jahre\n",
            "- Bildung: Mittel\n",
            "- Bundesland: Niederoesterreich\n",
            "- Impfen Ehestmöglich: Trifft eher zu\n",
            "- Verlasse mich lieber auf mein Immunsystem: Teils-teils\n",
            "- Selbstschutz durch Impfung: Trifft eher zu\n",
            "- Fremdschutz durch Impfung: Trifft eher zu\n",
            "- Behördeninformation: Trifft eher nicht zu\n",
            "Person:\n",
            "- Geschlecht: Maennlich\n",
            "- Alter: 40-49 Jahre\n",
            "- Bildung: Mittel\n",
            "- Bundesland: Niederoesterreich\n",
            "- Impfen Ehestmöglich: Trifft eher zu\n",
            "- Verlasse mich lieber auf mein Immunsystem: Teils-teils\n",
            "- Selbstschutz durch Impfung: Trifft eher zu\n",
            "- Fremdschutz durch Impfung: Teils-teils\n",
            "- Behördeninformation: Trifft eher nicht zu\n",
            "Person:\n",
            "- Geschlecht: Weiblich\n",
            "- Alter: 65+ Jahre\n",
            "- Bildung: Mittel\n",
            "- Bundesland: Niederoesterreich\n",
            "- Impfen Ehestmöglich: Teils-teils\n",
            "- Verlasse mich lieber auf mein Immunsystem: Teils-teils\n",
            "- Selbstschutz durch Impfung: Trifft eher zu\n",
            "- Fremdschutz durch Impfung: Teils-teils\n",
            "- Behördeninformation: Trifft eher nicht zu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the model form hugging faces\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=\"TheBloke/em_german_mistral_v01-GGUF\",\n",
        "    filename=\"em_german_mistral_v01.Q4_K_M.gguf\",\n",
        "    local_dir=\"/content\"  # save in colab enviroment\n",
        ")\n",
        "\n",
        "# initialize the model\n",
        "from llama_cpp import Llama\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_ctx=2048,\n",
        "    n_threads=8,\n",
        "    n_gpu_layers=0         # use cpu only\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYz9fPLJEW9t",
        "outputId": "6393b114-b000-4f4d-9e25-2818c4d56e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /content/em_german_mistral_v01.Q4_K_M.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = jphme_em_german_mistral_v01\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "print_info: file format = GGUF V2\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
            "init_tokenizer: initializing tokenizer for type 1\n",
            "load: control token:      2 '</s>' is not marked as EOG\n",
            "load: control token:      1 '<s>' is not marked as EOG\n",
            "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "load: special tokens cache size = 3\n",
            "load: token to piece cache size = 0.1637 MB\n",
            "print_info: arch             = llama\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 32768\n",
            "print_info: n_embd           = 4096\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 8\n",
            "print_info: n_rot            = 128\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_embd_head_k    = 128\n",
            "print_info: n_embd_head_v    = 128\n",
            "print_info: n_gqa            = 4\n",
            "print_info: n_embd_k_gqa     = 1024\n",
            "print_info: n_embd_v_gqa     = 1024\n",
            "print_info: f_norm_eps       = 0.0e+00\n",
            "print_info: f_norm_rms_eps   = 1.0e-05\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 14336\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 0\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 32768\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 7B\n",
            "print_info: model params     = 7.24 B\n",
            "print_info: general.name     = jphme_em_german_mistral_v01\n",
            "print_info: vocab type       = SPM\n",
            "print_info: n_vocab          = 32000\n",
            "print_info: n_merges         = 0\n",
            "print_info: BOS token        = 1 '<s>'\n",
            "print_info: EOS token        = 2 '</s>'\n",
            "print_info: UNK token        = 0 '<unk>'\n",
            "print_info: LF token         = 13 '<0x0A>'\n",
            "print_info: EOG token        = 2 '</s>'\n",
            "print_info: max token length = 48\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU\n",
            "load_tensors: layer   1 assigned to device CPU\n",
            "load_tensors: layer   2 assigned to device CPU\n",
            "load_tensors: layer   3 assigned to device CPU\n",
            "load_tensors: layer   4 assigned to device CPU\n",
            "load_tensors: layer   5 assigned to device CPU\n",
            "load_tensors: layer   6 assigned to device CPU\n",
            "load_tensors: layer   7 assigned to device CPU\n",
            "load_tensors: layer   8 assigned to device CPU\n",
            "load_tensors: layer   9 assigned to device CPU\n",
            "load_tensors: layer  10 assigned to device CPU\n",
            "load_tensors: layer  11 assigned to device CPU\n",
            "load_tensors: layer  12 assigned to device CPU\n",
            "load_tensors: layer  13 assigned to device CPU\n",
            "load_tensors: layer  14 assigned to device CPU\n",
            "load_tensors: layer  15 assigned to device CPU\n",
            "load_tensors: layer  16 assigned to device CPU\n",
            "load_tensors: layer  17 assigned to device CPU\n",
            "load_tensors: layer  18 assigned to device CPU\n",
            "load_tensors: layer  19 assigned to device CPU\n",
            "load_tensors: layer  20 assigned to device CPU\n",
            "load_tensors: layer  21 assigned to device CPU\n",
            "load_tensors: layer  22 assigned to device CPU\n",
            "load_tensors: layer  23 assigned to device CPU\n",
            "load_tensors: layer  24 assigned to device CPU\n",
            "load_tensors: layer  25 assigned to device CPU\n",
            "load_tensors: layer  26 assigned to device CPU\n",
            "load_tensors: layer  27 assigned to device CPU\n",
            "load_tensors: layer  28 assigned to device CPU\n",
            "load_tensors: layer  29 assigned to device CPU\n",
            "load_tensors: layer  30 assigned to device CPU\n",
            "load_tensors: layer  31 assigned to device CPU\n",
            "load_tensors: layer  32 assigned to device CPU\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:   CPU_Mapped model buffer size =  4165.37 MiB\n",
            "...............................................................................................\n",
            "llama_init_from_model: n_seq_max     = 1\n",
            "llama_init_from_model: n_ctx         = 2048\n",
            "llama_init_from_model: n_ctx_per_seq = 2048\n",
            "llama_init_from_model: n_batch       = 512\n",
            "llama_init_from_model: n_ubatch      = 512\n",
            "llama_init_from_model: flash_attn    = 0\n",
            "llama_init_from_model: freq_base     = 10000.0\n",
            "llama_init_from_model: freq_scale    = 1\n",
            "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
            "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
            "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
            "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
            "llama_init_from_model:        CPU compute buffer size =   164.01 MiB\n",
            "llama_init_from_model: graph nodes  = 1030\n",
            "llama_init_from_model: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'jphme_em_german_mistral_v01', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[\"Aussage: Behörden informieren hinreichend über Wirkungsweise der Impfstoffe\"].unique)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCfMtD_CJMud",
        "outputId": "345ea78c-b8ff-4615-ef3c-cc554e1f4230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Series.unique of 0                  Teils-teils\n",
            "1               Trifft eher zu\n",
            "2         Trifft eher nicht zu\n",
            "3         Trifft eher nicht zu\n",
            "4         Trifft eher nicht zu\n",
            "                ...           \n",
            "995                Teils-teils\n",
            "996                Teils-teils\n",
            "997                Teils-teils\n",
            "998        Trifft gar nicht zu\n",
            "999    Trifft voll und ganz zu\n",
            "Name: Aussage: Behörden informieren hinreichend über Wirkungsweise der Impfstoffe, Length: 1000, dtype: object>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MANUAL_EXAMPLES = [\n",
        "    {\n",
        "        \"prompt\": \"\"\"Person:\n",
        "- Geschlecht: Maennlich\n",
        "- Alter: 40-49 Jahre\n",
        "- Bildung: Mittel\n",
        "- Bundesland: Niederoesterreich\n",
        "- Impfen Ehestmöglich: Trifft eher zu\n",
        "- Verlasse mich lieber auf mein Immunsystem: Teils-teils\n",
        "- Selbstschutz durch Impfung: Trifft eher zu\n",
        "- Fremdschutz durch Impfung: Teils-teils\n",
        "- Behördeninformation: Trifft eher nicht zu\"\"\",\n",
        "        \"summary\": \"\"\"Ich bin ein zwischen 40 und 49 Jahre alter Mann und lebe in Wien. Ich werde mich wahrscheinlich ehestmögich impfen lassen.\n",
        "Ich verlasse mich teilweise auf mein Immunsystem. Ich denke, dass man sich durch die Impfung selber schützen kann aber weiß nicht, ob man auch andere schützen kann.\n",
        "Die Informationen der Behörde zur Impfung waren unzureichend.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"\"\"Person:\n",
        "- Geschlecht: Weiblich\n",
        "- Alter: 65+ Jahre\n",
        "- Bildung: Hoch\n",
        "- Bundesland: Wien\n",
        "- Impfen Ehestmöglich: Trifft voll zu\n",
        "- Verlasse mich lieber auf mein Immunsystem: Trifft gar nicht zu\n",
        "- Selbstschutz durch Impfung: Trifft voll zu\n",
        "- Fremdschutz durch Impfung: Trifft voll zu\n",
        "- Behördeninformation: Trifft voll zu\"\"\",\n",
        "        \"summary\": \"\"\"Ich bin eine in Wien lebende Frau und älter als 65 Jahre. Ich werde mich sicherlich ehestmöglich impfen lassen.\n",
        "Ich verlasse mich gar nicht auf mein Immunsystem und glaube sowohl an den Fremdschutz, als auch an den Selbstschutz der Impfung.\n",
        "Die Informationen der Behörden über die Impfung waren voll ausreichend.\"\"\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# category rules (they don't work perfectly tho..)\n",
        "CATEGORY_RULES = \"\"\"**Antwortrichtlinien:**\n",
        "1. \"Trifft voll zu\" → \"Ich werde definitiv...\" / \"Sicherlich...\"\n",
        "2. \"Trifft eher zu\" → \"Ich tendiere dazu...\" / \"Wahrscheinlich...\"\n",
        "3. \"Teils-teils\" → \"Ich bin mir unsicher...\" / \"Vielleicht...\"\n",
        "4. \"Trifft eher nicht zu\" → \"Ich werde nicht...\" / \"Eher nicht...\"\n",
        "5. \"Trifft gar nicht zu\" → \"Ich werde auf keinen Fall...\" / \"Gar nicht...\"'\n",
        "\"\"\"\n",
        "\n",
        "def format_prompt(prompt):\n",
        "    example_text = \"\"\n",
        "    for idx, ex in enumerate(MANUAL_EXAMPLES, 1):\n",
        "        example_text += f\"\"\"\n",
        "**Beispiel {idx}:**\n",
        "Daten:\n",
        "{ex['prompt']}\n",
        "Zusammenfassung: [/INST] {ex['summary']}[INST]\"\"\"\n",
        "\n",
        "    instruction = f\"\"\"[INST] <<SYS>>\n",
        "Stell dir vor, du bist diese Person. Beschreibe dich in 2-3 Sätzen, lasse keine Informationen aus:\n",
        "\n",
        "{CATEGORY_RULES}\n",
        "\n",
        "{example_text}\n",
        "<</SYS>>\n",
        "\n",
        "**Aktuelle Daten:**\n",
        "{prompt}\n",
        "**Deine Perspektive:** [/INST]\"\"\"\n",
        "\n",
        "    return instruction"
      ],
      "metadata": {
        "id": "Mxr7EZb1OR4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_persona(llm, prompt):\n",
        "    full_prompt = format_prompt(prompt)\n",
        "\n",
        "    output = llm(\n",
        "        full_prompt,\n",
        "        max_tokens=150,                           # lenght of the output\n",
        "        temperature=0.1,                          # more creativity besser: 0.5\n",
        "        top_p=0.95,                               # wider sampling: 0.95\n",
        "        top_k=15,                                 # more diverse besser: 40\n",
        "        repeat_penalty=1.25,                      # higher penalty = less repeating phrases or sentences: 1.20\n",
        "        stop=[\"\\n\\n\", \"</s>\", \"[INST]\"],          # \"\\n\\n\" = Double Newline, stop the model at a logical paragraph end // \"</s>\" = End-of-Sequence Token // \"[INST]\" =  Instruction Marker\n",
        "        echo=False                                # don't return prompt\n",
        "    )\n",
        "\n",
        "    # Handle empty response\n",
        "    response = output['choices'][0]['text'].strip()\n",
        "\n",
        "    # fallback if empty\n",
        "    if not response:\n",
        "        return \"Zusammenfassung konnte nicht generiert werden.\"\n",
        "\n",
        "    return response.split(\"Zusammenfassung:\")[-1].strip()\n"
      ],
      "metadata": {
        "id": "LvW3LGZ-DeX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# randomly sample from the dataset\n",
        "sample_size = min(10, len(dataset))\n",
        "\n",
        "valid_count = 0  # counting answers\n",
        "\n",
        "if sample_size > 0:\n",
        "    random.seed(123)\n",
        "    sampled_indices = random.sample(range(len(dataset)), sample_size)\n",
        "\n",
        "    for idx in sampled_indices:\n",
        "        if valid_count >= 3:\n",
        "            break\n",
        "\n",
        "        prompt = dataset[idx][\"prompt\"]\n",
        "        print(f\"\\n=== Prompt {idx+1} ===\")\n",
        "        print(prompt)\n",
        "\n",
        "        for attempt in range(3):\n",
        "            response = generate_persona(llm, prompt)\n",
        "            if response != \"Zusammenfassung konnte nicht generiert werden.\":\n",
        "                print(\"\\nGenerierte Persona:\")\n",
        "                print(f\"Persona: {response}\")\n",
        "                print(\"=\"*50)\n",
        "                valid_count += 1\n",
        "                break  # next prompt\n",
        "\n",
        "        if valid_count >=5:\n",
        "            break\n",
        "\n",
        "    if valid_count <3:\n",
        "        print(f\"\\nNur {valid_count} gültige Personas gefunden!\")\n",
        "else:\n",
        "    print(\"Dataset ist leer!\")\n",
        "\n",
        "\n",
        "\"\"\"without sampling:\n",
        "for i in range(10):\n",
        "    prompt = dataset[i][\"prompt\"]\n",
        "    response = generate_persona(llm, prompt)\n",
        "\n",
        "    if response == \"Zusammenfassung konnte nicht generiert werden.\":\n",
        "        continue  # Überspringe diesen Eintrag, wenn keine gültige Antwort\n",
        "\n",
        "    print(f\"Prompt {i+1}:\")\n",
        "    print(prompt)\n",
        "    print(\"\\nGenerierte Persona:\")\n",
        "    print(response)\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EzQDfTxqH05f",
        "outputId": "a1bb4c0e-f738-4531-aac1-cea7c4ae5a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Prompt 4 ===\n",
            "Person:\n",
            "- Geschlecht: Maennlich\n",
            "- Alter: 40-49 Jahre\n",
            "- Bildung: Mittel\n",
            "- Bundesland: Niederoesterreich\n",
            "- Impfen Ehestmöglich: Trifft eher zu\n",
            "- Verlasse mich lieber auf mein Immunsystem: Teils-teils\n",
            "- Selbstschutz durch Impfung: Trifft eher zu\n",
            "- Fremdschutz durch Impfung: Teils-teils\n",
            "- Behördeninformation: Trifft eher nicht zu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 707 prefix-match hit, remaining 129 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =  400784.07 ms\n",
            "llama_perf_context_print: prompt eval time =   66931.20 ms /   129 tokens (  518.85 ms per token,     1.93 tokens per second)\n",
            "llama_perf_context_print:        eval time =   96504.56 ms /   110 runs   (  877.31 ms per token,     1.14 tokens per second)\n",
            "llama_perf_context_print:       total time =  163602.84 ms /   239 tokens\n",
            "Llama.generate: 707 prefix-match hit, remaining 131 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generierte Persona:\n",
            "Persona: Ich bin ein zwischen 40 und 49 Jahre alter Mann und lebe in Wien. Ich werde mich wahrscheinlich ehestmögich impfen lassen.\n",
            "Ich verlasse mich teilweise auf mein Immunsystem. Ich denke, dass man sich durch die Impfung selber schützen kann aber weiß nicht, ob man auch andere schützen kann.\n",
            "Die Informationen der Behörde zur Impfung waren unzureichend.\n",
            "==================================================\n",
            "\n",
            "=== Prompt 18 ===\n",
            "Person:\n",
            "- Geschlecht: Weiblich\n",
            "- Alter: 40-49 Jahre\n",
            "- Bildung: Mittel\n",
            "- Bundesland: Oberoesterreich\n",
            "- Impfen Ehestmöglich: Trifft gar nicht zu\n",
            "- Verlasse mich lieber auf mein Immunsystem: Trifft voll und ganz zu\n",
            "- Selbstschutz durch Impfung: Trifft gar nicht zu\n",
            "- Fremdschutz durch Impfung: Trifft gar nicht zu\n",
            "- Behördeninformation: Trifft gar nicht zu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =  400784.07 ms\n",
            "llama_perf_context_print: prompt eval time =   63720.17 ms /   131 tokens (  486.41 ms per token,     2.06 tokens per second)\n",
            "llama_perf_context_print:        eval time =   71850.85 ms /   102 runs   (  704.42 ms per token,     1.42 tokens per second)\n",
            "llama_perf_context_print:       total time =  135697.53 ms /   233 tokens\n",
            "Llama.generate: 716 prefix-match hit, remaining 127 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generierte Persona:\n",
            "Persona: Ich bin eine zwischen 40 und 49 Jahre alte Frau aus Oberoesterreich. Ich werde mich nicht ehestmöglich impfen lassen, denn ich verlaße voll und ganz auf mein Immunsystem und glaube gar nicht an die Wirksamkeit der Impfung zum Selbst- oder Fremdschutz.\n",
            "Die Informationen der Behörden zur Impfung waren mir unzureichend.\n",
            "==================================================\n",
            "\n",
            "=== Prompt 6 ===\n",
            "Person:\n",
            "- Geschlecht: Weiblich\n",
            "- Alter: 50-64 Jahre\n",
            "- Bildung: Mittel\n",
            "- Bundesland: Niederoesterreich\n",
            "- Impfen Ehestmöglich: Trifft voll und ganz zu\n",
            "- Verlasse mich lieber auf mein Immunsystem: Trifft gar nicht zu\n",
            "- Selbstschutz durch Impfung: Trifft voll und ganz zu\n",
            "- Fremdschutz durch Impfung: Trifft voll und ganz zu\n",
            "- Behördeninformation: Trifft eher zu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =  400784.07 ms\n",
            "llama_perf_context_print: prompt eval time =   70214.31 ms /   127 tokens (  552.87 ms per token,     1.81 tokens per second)\n",
            "llama_perf_context_print:        eval time =   89887.22 ms /   124 runs   (  724.90 ms per token,     1.38 tokens per second)\n",
            "llama_perf_context_print:       total time =  160257.76 ms /   251 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generierte Persona:\n",
            "Persona: Ich bin eine zwischen 50 und 64 Jahre alte Frau aus Niederoesterreich. Ich werde sicherlich ehestmöglich impfen lassen, weil ich glaube, dass sowohl der Selbstschutz als auch der Fremdschutz durch die Impfung sehr wichtig sind.\n",
            "Ich verlasse gar nicht auf mein Immunsystem und denke, dass man sich selbst schützen kann aber unsicher ist, ob man andere damit schützt. Die Informationen der Behörde waren zureichend.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'without sampling:\\nfor i in range(10):\\n    prompt = dataset[i][\"prompt\"]\\n    response = generate_persona(llm, prompt)\\n\\n    if response == \"Zusammenfassung konnte nicht generiert werden.\":\\n        continue  # Überspringe diesen Eintrag, wenn keine gültige Antwort\\n\\n    print(f\"Prompt {i+1}:\")\\n    print(prompt)\\n    print(\"\\nGenerierte Persona:\")\\n    print(response)\\n    print(\"\\n\" + \"=\"*50 + \"\\n\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further optimization is certainly achievable. For a lightweight model running solely on CPU, the current results are somewhat reasonable (parameters such as top_k, temperature, etc., are very important and drastically change how well the sentences are created from the prompt). A more powerful model would deliver much faster and better outcomes with less reliance on extensive prompt engineering, enabling the generation of a greater number of personas with significantly more variables included in the prompt. This enhancement could be realized through local GPU utilization, investment in API quotas, or the utilization of outsourced GPU resources."
      ],
      "metadata": {
        "id": "sHfocVJDjF4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Option 2: Utilizing Google Gemini API (API-Key necessary)\n",
        "\n",
        "Results are much better but the generation only possible with enough credits"
      ],
      "metadata": {
        "id": "CJE5tyDnIPxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install google-generativeai"
      ],
      "metadata": {
        "id": "jrKaDDH_U_ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import time"
      ],
      "metadata": {
        "id": "B9oJGEw2ZxeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "genai.configure(api_key=userdata.get(\"gemini_key\"))"
      ],
      "metadata": {
        "id": "FTfR464_Z2mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "    if 'gemini' in m.name:\n",
        "        print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "tUOvIkhsbW26",
        "outputId": "e4147f89-35f0-4efa-9b5a-b945728a1d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.5-flash-preview-04-17\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/gemini-2.0-flash-live-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize model\n",
        "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
        "\n",
        "# this function uses the prompt which was generated with create_prompts_directly\n",
        "\n",
        "def generate_persona_gemini(prompt):\n",
        "    system_instruction = \"\"\"Erstelle eine Personenbeschreibung in 2-3 Sätzen auf Deutsch:\n",
        "1. Demographie (Alter, Geschlecht, Bildung, Bundesland)\n",
        "2. Impfverhalten und Begründungen\n",
        "\n",
        "Beispielantwort:\n",
        "Ich bin eine 65-jährige Frau mit hoher Bildung aus Vorarlberg. Ich stehe Impfungen eher skeptisch gegenüber und werde mich wahrscheinlich nicht ehestmöglich impfen lassen.\n",
        "Ich vertraue zwar nicht vollständig auf mein Immunsystem, bin mir aber unsicher, ob Impfungen wirklich Selbst- oder Fremdschutz bieten. Die Informationen der Behörden empfinde ich als teilweise unzureichend.\"\"\"\n",
        "\n",
        "    full_prompt = f\"{system_instruction}\\n\\nAktuelle Daten:\\n{prompt}\" # use the prompt which was generated with create_prompts_directly\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(\n",
        "            full_prompt,\n",
        "            generation_config={\n",
        "                \"temperature\": 0.3,  # Less randomness\n",
        "                \"max_output_tokens\": 200,\n",
        "            }\n",
        "        )\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# process first 5 entries with rate limiting\n",
        "for i in range(5):\n",
        "    prompt = dataset[i][\"prompt\"]\n",
        "    print(f\"Processing prompt {i+1}...\")\n",
        "\n",
        "    persona = generate_persona_gemini(prompt)\n",
        "    if persona:\n",
        "        print(\"\\nGenerated Persona:\")\n",
        "        print(persona)\n",
        "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    time.sleep(1)  # rate limiting (1 request/sec)\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "6GQTqPK4Z8s7",
        "outputId": "37523a18-0abc-4ef1-f39b-71d950bb8a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing prompt 1...\n",
            "\n",
            "Generated Persona:\n",
            "Ich bin ein über 65-jähriger Mann mit hoher Bildung aus Vorarlberg.  Obwohl ich mich nicht ausschließlich auf mein Immunsystem verlasse, bin ich beim Thema Impfen noch unentschlossen und werde mich daher eher nicht ehestmöglich impfen lassen.  Der Nutzen von Impfungen hinsichtlich Selbst- und Fremdschutz ist für mich nicht eindeutig geklärt, und die Informationen der Behörden empfinde ich als teilweise unzureichend.\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "Processing prompt 2...\n",
            "\n",
            "Generated Persona:\n",
            "Ich bin eine Frau zwischen 50 und 64 Jahren mit niedriger Bildung aus Oberösterreich. Ich lasse mich ehestmöglich impfen, da ich nicht auf mein Immunsystem vertrauen möchte und an den Selbst- und Fremdschutz durch Impfungen glaube.  Die Informationen der Behörden halte ich für ausreichend.\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "Processing prompt 3...\n",
            "\n",
            "Generated Persona:\n",
            "Ich bin ein Mann zwischen 40 und 49 Jahren mit mittlerer Bildung aus Niederösterreich. Ich lasse mich eher ehestmöglich impfen, da ich vom Selbst- und Fremdschutz durch die Impfung überzeugt bin.  Die Informationen der Behörden finde ich allerdings eher unzureichend.\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "Processing prompt 4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 203.60ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "Processing prompt 5...\n",
            "Error: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Option 2: Utilizing Hugging Face Inference API (API-Key necessary)\n",
        "\n",
        "Unfortunately, the last output was overwritten due to missing credits for the text generation. The results are comparable to those from the Gemini API."
      ],
      "metadata": {
        "id": "UQI5Qp9WdDOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import InferenceClient"
      ],
      "metadata": {
        "id": "DKtaaJoBdPNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login() # check if the token works (needs to be a \"read\" token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "79006067b7234ba298489a7f82b924a1",
            "8d723c105c7e4cdca80b7c354e1157fc",
            "a6fa6467f7e4403e837dcbfa4813157c",
            "1fec4729d8924a25b930ad83c7224fd6",
            "33dd6349c28541a887780310aaacba92",
            "d2a7868813b54309869689c5eb1e10e1",
            "72925d2d7265423391a9816a77390354",
            "6a4b8a997d2a4fea93f0bf32af65d99c",
            "07038ec358eb4561a31bb803b81093cc",
            "d3aaaa70ba3a4db19114a60499601049",
            "c07fa4a998e04411a91ecfb172e76a12",
            "ba502df6f4ee4d66a05b67b7153f6461",
            "3e9a0df988dd44e6a606d44709df1e10",
            "ca7e3482c4a647abb0cbc6c3fde304cc",
            "50de3d3692a949d5925da25aeb6117e6",
            "9d624ca8663741e88dcf1904d9306477",
            "b96f09f7228145279a4dce49aa9a59ee",
            "bd89c4a4a5f244df84da75c1eeec97a4",
            "232359131bb54d53ad5b11564f65cb09",
            "511882d0d4704d8298f7ee0ddeeda124"
          ]
        },
        "id": "Nw01INqth-3z",
        "outputId": "76a6f409-86a9-4ed1-f7f4-a6aa7622570d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79006067b7234ba298489a7f82b924a1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_client = InferenceClient(\n",
        "    model=\"HuggingFaceH4/zephyr-7b-beta\",  # German-focused model\n",
        "    token=True,  # Uses the token from login()\n",
        "    timeout=300\n",
        ")\n"
      ],
      "metadata": {
        "id": "VG6iwbQWnV_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_persona(prompt: str) -> str:\n",
        "    system_msg = \"\"\"<|system|>\n",
        "Erstelle eine Personenbeschreibung in 2-3 Sätzen:\n",
        "1. Demographie (Alter, Geschlecht, Bildung, Bundesland)\n",
        "2. Impfverhalten und Begründungen\n",
        "</s>\n",
        "\"\"\"\n",
        "    user_msg = f\"<|user|>\\n{prompt}\\n</s>\\n<|assistant|>\"\n",
        "\n",
        "    try:\n",
        "        response = llm_client.text_generation(\n",
        "            prompt=system_msg + user_msg,\n",
        "            max_new_tokens=150,\n",
        "            temperature=0.3,\n",
        "            repetition_penalty=1.15,\n",
        "            stop_sequences=[\"</s>\", \"\\n\\n\"]\n",
        "        )\n",
        "        return response.split(\"<|assistant|>\")[-1].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Generation failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Step 4: Test with your data\n",
        "for i in range(5):\n",
        "    prompt = dataset[i][\"prompt\"]\n",
        "    print(f\"Prompt {i+1}:\")\n",
        "    print(generate_persona(prompt))\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95ASfdZFaSCa",
        "outputId": "79b017ec-163f-405b-a388-e0afa5081900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt 1:\n",
            "Generation failed: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/HuggingFaceH4/zephyr-7b-beta (Request ID: Root=1-6811d5a6-7cd05406270b3469634fea2c;278ea669-4801-4907-9da6-48b059c977e2)\n",
            "\n",
            "You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\n",
            "\n",
            "==================================================\n",
            "Prompt 2:\n",
            "Generation failed: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/HuggingFaceH4/zephyr-7b-beta (Request ID: Root=1-6811d5a6-7b2ac1746d88500a19480148;a6b6d904-9034-41e4-ad69-40173d273b9f)\n",
            "\n",
            "You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\n",
            "\n",
            "==================================================\n",
            "Prompt 3:\n",
            "Generation failed: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/HuggingFaceH4/zephyr-7b-beta (Request ID: Root=1-6811d5a6-24fd498824e2fed34a2ed2aa;31c5bc36-27e2-4ff3-864a-c5132f5cf3b8)\n",
            "\n",
            "You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\n",
            "\n",
            "==================================================\n",
            "Prompt 4:\n",
            "Generation failed: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/HuggingFaceH4/zephyr-7b-beta (Request ID: Root=1-6811d5a6-60cbe54f7da813724c8c1c48;cf64fb68-2406-43bc-8172-08e881541d7c)\n",
            "\n",
            "You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\n",
            "\n",
            "==================================================\n",
            "Prompt 5:\n",
            "Generation failed: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/HuggingFaceH4/zephyr-7b-beta (Request ID: Root=1-6811d5a6-3f5ef55678bdb368241033d2;1823b31a-99a9-4d61-8f34-4ec6dc088e27)\n",
            "\n",
            "You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.\n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Possible options for fine-tuning"
      ],
      "metadata": {
        "id": "2Rj_CLN9_A9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\" preparing the data for our prompt structure from before (there would probably be much more variables in a real scenario):\n",
        "\n",
        "- Geschlecht: Maennlich\n",
        "- Alter: 65+ Jahre\n",
        "- Bildung: Hoch\n",
        "- Bundesland: Vorarlberg\n",
        "- Impfen Ehestmöglich: Trifft eher nicht zu\n",
        "- Verlasse mich lieber auf mein Immunsystem: Trifft gar nicht zu\n",
        "- Selbstschutz durch Impfung: Teils-teils\n",
        "- Fremdschutz durch Impfung: Teils-teils\n",
        "- Behördeninformation: Teils-teils\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def format_structured_data(examples):\n",
        "    return {\n",
        "        \"text\": f\"[INST] <<SYS>>\\nErstelle eine Personenbeschreibung basierend auf:\\n{examples['prompt']}\\n<</SYS>>\\n[/INST]\\n{examples['response']}\"\n",
        "    }\n",
        "\n",
        "# load model with QLoRA\n",
        "model_id = \"LeoLM/leo-mistral-7b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    load_in_4bit=True, # this activates QLoRA\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# prepare dataset\n",
        "train_dataset = dataset.map(format_structured_data, remove_columns=[\"prompt\", \"response\"])\n",
        "tokenized_dataset = train_dataset.map(\n",
        "    lambda x: tokenizer(x[\"text\"], truncation=True, max_length=512),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "# since LoRA and QLoRA\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"], # query, key and value vectors, probably needed for persona generation\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# training\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=2,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        output_dir=\"./results\",\n",
        "    ),\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=lambda data: {\"input_ids\": torch.stack([f[\"input_ids\"] for f in data])}\n",
        ")\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "ThEMERv6AcJP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}